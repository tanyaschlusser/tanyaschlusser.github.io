<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tanya Schlusser (Posts about bayesian)</title><link>https://tanyaschlusser.github.io/</link><description></description><atom:link href="https://tanyaschlusser.github.io/categories/bayesian.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2018 &lt;a href="mailto:tanya@tickel.net"&gt;Tanya Schlusser&lt;/a&gt; </copyright><lastBuildDate>Mon, 30 Jul 2018 23:08:27 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>MCMC and the Ising Model</title><link>https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/</link><dc:creator>Tanya Schlusser</dc:creator><description>&lt;figure&gt;&lt;img src="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/spins.png"&gt;&lt;/figure&gt; &lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;span class="vocabulary" title="A sequence of statistical outcomes in which each step is statistically independent from all of the prior steps."&gt;Markov-Chain&lt;/span&gt;
&lt;span class="vocabulary" title="A computer simulation technique using pseudo-random numbers to simulate random events."&gt;Monte Carlo&lt;/span&gt; (MCMC) methods are a category of numerical technique used in Bayesian statistics. They numerically estimate the distribution of a variable (the &lt;span class="vocabulary" title="The prior times the likelihood,  normalized, is the posterior distribution: the probability distribution of the target variable after incorporating the observed data."&gt;posterior&lt;/span&gt;) given two other distributions: the &lt;span class="vocabulary" title="A distribution that represents existing knowledge of a system. Often people choose a uniform (flat) distribution; or else something that is the known conjugate prior of a desired posterior distribution."&gt;prior&lt;/span&gt; and the &lt;span class="vocabulary" title="A special name for the probability mass (or density) function when you fix the random variable (e.g. `x`) and integrate over the parameters (e.g. `mu` and `theta`). It's renamed 'likelihood' just to make that swap explicit when talking about it. The integral over the parameters may not equal one so you have to normalize."&gt;likelihood function&lt;/span&gt;, and are useful when direct integration of the likelihood function is not tractable.&lt;/p&gt;
&lt;p&gt;I am new to Bayesian statistics, but became interested in the approach partly from exposure to the &lt;a href="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/"&gt;PyMC3 library&lt;/a&gt;, and partly from FiveThirtyEight's promoting it in a &lt;a href="https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/"&gt;commentary&lt;/a&gt; soon after the time of the P-hacking scandals a few years back (&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/22006061"&gt;Simmons et. al.&lt;/a&gt; coin 'p-hacking' in 2011, and &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000"&gt;Head et. al.&lt;/a&gt; quantify the scale of the issue in 2014).&lt;/p&gt;
&lt;p&gt;Until the 1980's, it was not realistic to use Bayesian techniques except when analytic solutions were possible. (Here's Wikipedia's &lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions"&gt;list of analytic options&lt;/a&gt;. They're still useful.) MCMC opens up more options.&lt;/p&gt;
&lt;p&gt;The Python library &lt;a href="https://docs.pymc.io/"&gt;pymc3&lt;/a&gt; provides a suite of modern Bayesian tools: both MCMC algorithms and variational inference. One of its core contributors, Thomas Wiecki, wrote a blog post entitled &lt;a href="https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/"&gt;MCMC sampling for dummies&lt;/a&gt;, which was the inspiration for this post. It was enthusiastically received, and cited by people I follow as the best available explanation of MCMC. To my dismay, I didn't understand it; probably because he comes from a stats background and I come from engineering. This post is for people like me.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/"&gt;Read more&lt;/a&gt; (24 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bayesian</category><category>mcmc</category><category>pymc3</category><guid>https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/</guid><pubDate>Sun, 29 Jul 2018 05:00:42 GMT</pubDate></item></channel></rss>