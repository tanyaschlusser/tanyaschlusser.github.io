<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tanya Schlusser (Posts about Methods)</title><link>https://tanyaschlusser.github.io/</link><description></description><atom:link href="https://tanyaschlusser.github.io/categories/cat_methods.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:tanya@tickel.net"&gt;Tanya Schlusser&lt;/a&gt; </copyright><lastBuildDate>Mon, 10 Sep 2018 14:48:06 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Bayesian updating and the NFL</title><link>https://tanyaschlusser.github.io/posts/bayesian-updating-and-the-nfl/</link><dc:creator>Tanya Schlusser</dc:creator><description>&lt;figure&gt;&lt;img src="https://tanyaschlusser.github.io/posts/bayesian-updating-and-the-nfl/home_spreads.png"&gt;&lt;/figure&gt; &lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It's football season again, hooray! Every year for my friends' football pool I try out a different algorithm. Invariably, my picks are around 60% accurate. Not terrible, but according to NFL Pickwatch (&lt;a href="https://web.archive.org/web/20180811195907/http://nflpickwatch.com/"&gt;archive&lt;/a&gt;, &lt;a href="https://nflpickwatch.com/"&gt;current season&lt;/a&gt;), the best pickers get to 68 or 69%. So, an amazing performance—my upper bound—is just under 70%, and the lower bound for a competitive model—the FiveThirtyEight baseline—is 60%.&lt;/p&gt;
&lt;p&gt;I've been modeling NFL outcomes for a couple of years, and running linear (predicting point spread) and logistic (predicting win probability) regressions given various team and player data. My best year so far incorporated the Vegas spread into the model, and my biggest disaster so far was an aggressive lasso model on every player in every offensive line, with team defenses lumped as a group. Attempting to track &lt;a href="https://www.pro-football-reference.com/players/injuries.htm"&gt;injuries&lt;/a&gt;, suspensions, and other changes to the starting lineup was not sustainable for the amount of time I wanted to spend.&lt;/p&gt;
&lt;p&gt;Enter Nate Silver's awesome &lt;span class="vocabulary" title="Arpad Elo was a Hungarian-American physics professor who invented the system to rank chess players. Silver adapted it for Football, baseball, and most of the other sports on FiveThirtyEight."&gt;&lt;a href="https://fivethirtyeight.com/features/how-our-2017-nfl-predictions-work/"&gt;NFL Elo rankings&lt;/a&gt;&lt;/span&gt;, the aspirational target for this year. What's impressive is that he gets something like 60% accuracy out of literally no information but home field advantage and past scores. I particularly love that it updates weekly to incorporate the new information—this immediately says "Bayesian" and in fact is a lot how people using their intuition are making their picks anyway. A system like his—but with a more straightforward Bayesian model—is the goal of this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tanyaschlusser.github.io/posts/bayesian-updating-and-the-nfl/"&gt;Read more&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bayesian</category><category>feature engineering</category><category>iterative</category><guid>https://tanyaschlusser.github.io/posts/bayesian-updating-and-the-nfl/</guid><pubDate>Sun, 09 Sep 2018 05:00:42 GMT</pubDate></item><item><title>MCMC and the Ising Model</title><link>https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/</link><dc:creator>Tanya Schlusser</dc:creator><description>&lt;figure&gt;&lt;img src="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/spins.png"&gt;&lt;/figure&gt; &lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;span class="vocabulary" title="A sequence of statistical outcomes in which each step is statistically independent from all of the prior steps."&gt;Markov-Chain&lt;/span&gt;
&lt;span class="vocabulary" title="A computer simulation technique using pseudo-random numbers to simulate random events."&gt;Monte Carlo&lt;/span&gt; (MCMC) methods are a category of numerical technique used in Bayesian statistics. They numerically estimate the distribution of a variable (the &lt;span class="vocabulary" title="The prior times the likelihood,  normalized, is the posterior distribution: the probability distribution of the target variable after incorporating the observed data."&gt;posterior&lt;/span&gt;) given two other distributions: the &lt;span class="vocabulary" title="A distribution that represents existing knowledge of a system. Often people choose a uniform (flat) distribution; or else something that is the known conjugate prior of a desired posterior distribution."&gt;prior&lt;/span&gt; and the &lt;span class="vocabulary" title="A special name for the probability mass (or density) function when you fix the random variable (e.g. `x`) and integrate over the parameters (e.g. `mu` and `theta`). It's renamed 'likelihood' just to make that swap explicit when talking about it. The integral over the parameters may not equal one so you have to normalize."&gt;likelihood function&lt;/span&gt;, and are useful when direct integration of the likelihood function is not tractable.&lt;/p&gt;
&lt;p&gt;I am new to Bayesian statistics, but became interested in the approach partly from exposure to the &lt;a href="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/"&gt;PyMC3 library&lt;/a&gt;, and partly from FiveThirtyEight's promoting it in a &lt;a href="https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/"&gt;commentary&lt;/a&gt; soon after the time of the p-hacking scandals a few years back (&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/22006061"&gt;Simmons et. al.&lt;/a&gt; coin 'p-hacking' in 2011, and &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000"&gt;Head et. al.&lt;/a&gt; quantify the scale of the issue in 2014).&lt;/p&gt;
&lt;p&gt;Until the 1980's, it was not realistic to use Bayesian techniques except when analytic solutions were possible. (Here's Wikipedia's &lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions"&gt;list of analytic options&lt;/a&gt;. They're still useful.) MCMC opens up more options.&lt;/p&gt;
&lt;p&gt;The Python library &lt;a href="https://docs.pymc.io/"&gt;pymc3&lt;/a&gt; provides a suite of modern Bayesian tools: both MCMC algorithms and variational inference. One of its core contributors, Thomas Wiecki, wrote a blog post entitled &lt;a href="https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/"&gt;MCMC sampling for dummies&lt;/a&gt;, which was the inspiration for this post. It was enthusiastically received, and cited by people I follow as the best available explanation of MCMC. To my dismay, I didn't understand it; probably because he comes from a stats background and I come from engineering. This post is for people like me.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/"&gt;Read more&lt;/a&gt; (24 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bayesian</category><category>mcmc</category><category>pymc3</category><guid>https://tanyaschlusser.github.io/posts/mcmc-and-the-ising-model/</guid><pubDate>Sun, 29 Jul 2018 05:00:42 GMT</pubDate></item></channel></rss>