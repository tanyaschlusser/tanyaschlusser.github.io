{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"vocabulary\" title=\"A sequence of statistical outcomes in which each step is statistically independent from all of the prior steps.\">Markov-Chain</span>\n",
    "<span class=\"vocabulary\" title=\"A computer simulation technique using pseudo-random numbers to simulate random events.\">Monte Carlo</span> (MCMC) methods are a category of numerical technique used in Bayesian statistics. They numerically estimate the distribution of a variable (the <span class=\"vocabulary\" title=\"The prior times the likelihood,  normalized, is the posterior distribution: the probability distribution of the target variable after incorporating the observed data.\">posterior</span>) given two other distributions: the <span class=\"vocabulary\" title=\"A distribution that represents existing knowledge of a system. Often people choose a uniform (flat) distribution; or else something that is the known conjugate prior of a desired posterior distribution.\">prior</span> and the <span class=\"vocabulary\" title=\"A special name for the probability mass (or density) function when you fix the random variable (e.g. `x`) and integrate over the parameters (e.g. `mu` and `theta`). It's renamed 'likelihood' just to make that swap explicit when talking about it. The integral over the parameters may not equal one so you have to normalize.\">likelihood function</span>, and are useful when direct integration of the likelihood function is not tractable.\n",
    "\n",
    "I am new to Bayesian statistics, but became interested in the approach partly from exposure to the <a href=\"\">PyMC3 library</a>, and partly from FiveThirtyEight's promoting it in a <a href=\"https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/\">commentary</a> soon after the time of the P-hacking scandals a few years back (<a href=\"https://www.ncbi.nlm.nih.gov/pubmed/22006061\">Simmons et. al.</a> coin 'p-hacking' in 2011, and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000\">Head et. al.</a> quantify the scale of the issue in 2014).\n",
    "\n",
    "\n",
    "Until the 1980's, it was not realistic to use Bayesian techniques except when analytic solutions were possible. (Here's Wikipedia's [list of analytic options](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions). They're still useful.) MCMC opens up more options.\n",
    "\n",
    "The Python library [pymc3](https://docs.pymc.io/) provides a suite of modern Bayesian tools: both MCMC algorithms and variational inference. One of its core contributors, Thomas Wiecki, wrote a blog post entitled [MCMC sampling for dummies](https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/), which was the inspiration for this post. It was enthusiastically received, and cited by people I follow as the best available explanation of MCMC. To my dismay, I didn't understand it; probably because he comes from a stats background and I come from engineering. This post is for people like me.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "### MCMC for dumber dummies (i.e. engineers like me)\n",
    "\n",
    "My first exposure to MCMC was via the Ising model. This post is for people like me who may remember the term \"MCMC\" from school and want to start from there to gain a working understanding of Bayesian numerical methods. If engineering concepts are familiar, this should be an accessible read.\n",
    "\n",
    "If you have simulated the Ising model, you've already implemented the original MCMC algorithm: the Metropolis-Hastings Method. If not, the next section briefly describes both MCMC and the Ising model.\n",
    "\n",
    "## Background\n",
    "\n",
    "The Metropolis-Hastings method was first developed right after World War II, when Metropolis and his team were exploring the physics of fission and fusion for use in a thermonuclear weapon. They published the method, to be used in general statistical mechanics applications, in 1953. Teachers use the Ising model to teach the Metropolis-Hastings method because it's less complicated than modeling anything with moving atoms.\n",
    "\n",
    "### Thermodynamic ensembles as an analogy for MCMC\n",
    "\n",
    "Thermodynamic ensembles are a concept from engineering: Imagine any starting state we want in a system, and step it forward in time, allowing for randomness. If the system goes long enough, the randomness will even out and we can get an equilibrium value by averaging over the ensemble.\n",
    "\n",
    "This example of mixing is how I visualize what's happening in Markov Chain Monte Carlo. Except instead of advancing in time, like here, the model advances a system through probability space toward more probable configurations.\n",
    "\n",
    "<canvas id=\"mcmc-ising-random-motion\" width=\"250px\" height=\"250px\" style=\"display:block;margin:1em auto 0;\">\n",
    "    Even when the initial conditions are not random,\n",
    "    like these two different colored balls balls placed\n",
    "    in two separete corners of a box, the eventual long\n",
    "    term average of the system looks like an even mixture.\n",
    "</canvas>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "//%%javascript\n",
    "(function(){\n",
    "    var canvas = document.getElementById(\"mcmc-ising-random-motion\");\n",
    "    var ctx = canvas.getContext('2d');\n",
    "    var twoPI = Math.PI * 2;\n",
    "    var i;\n",
    "    canvas.setAttribute(\"role\", \"image\");\n",
    "    canvas.setAttribute(\"aria-label\", canvas.innerHTML);\n",
    "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "    \n",
    "    const molecule = ({x=0, y=0, color=\"#dff\", radius=10} = {}) => ({\n",
    "      x, y, color, r: radius, x0: x, y0: y,\n",
    "      xdot: Math.random() * 6,\n",
    "      ydot: Math.random() * 6,\n",
    "      handleCollision (other) {\n",
    "        if (other === undefined) {  // Walls\n",
    "            if (this.x < this.r) {\n",
    "                this.x = this.r + this.r - this.x;\n",
    "                this.xdot = -this.xdot;        \n",
    "            } else if (this.x > ctx.canvas.width - this.r) {\n",
    "                this.x = this.x - this.r;\n",
    "                this.xdot = -this.xdot;\n",
    "            }\n",
    "            if (this.y < this.r) {\n",
    "                this.y = this.r + this.r - this.y;\n",
    "                this.ydot = -this.ydot;\n",
    "            } else if (this.y > ctx.canvas.height - this.r) {\n",
    "                this.y = this.y - this.r;\n",
    "                this.ydot = -this.ydot;\n",
    "            }\n",
    "        } else {  // other molecule\n",
    "            var dx = this.x - other.x;\n",
    "            var dy = this.y - other.y;\n",
    "            if (Math.pow(dx, 2) + Math.pow(dy, 2) < 4 * Math.pow(this.r, 2)) {\n",
    "                // collision\n",
    "                var xdenom = Math.abs(this.xdot) + Math.abs(other.xdot);\n",
    "                var ydenom = Math.abs(this.ydot) + Math.abs(other.ydot);\n",
    "                var tmp = other.xdot;\n",
    "                other.xdot = this.xdot;\n",
    "                this.xdot = tmp;\n",
    "                tmp = other.ydot;\n",
    "                other.ydot = this.ydot;\n",
    "                this.ydot = tmp;\n",
    "                this.x = this.x + dx/2 * Math.abs(this.xdot) / xdenom;\n",
    "                this.y = this.y + dy/2 * Math.abs(this.ydot) / ydenom;\n",
    "                other.x = other.x - dx/2 * Math.abs(other.xdot) / xdenom;\n",
    "                other.y = other.y - dy/2 * Math.abs(other.ydot) / ydenom;\n",
    "            }\n",
    "        }\n",
    "      },\n",
    "      step() {\n",
    "          this.x = this.x + this.xdot;\n",
    "          this.y = this.y + this.ydot;\n",
    "      },\n",
    "      draw() {\n",
    "        ctx.save();\n",
    "        ctx.fillStyle = this.color;\n",
    "        ctx.strokeStyle = 'black';\n",
    "        ctx.beginPath();\n",
    "        ctx.moveTo(this.x + this.r, this.y);\n",
    "        ctx.arc(this.x, this.y, this.r, 0, twoPI, 0);\n",
    "        ctx.fill();      \n",
    "        ctx.stroke();\n",
    "        ctx.closePath();\n",
    "        ctx.restore();\n",
    "      }\n",
    "    });\n",
    "\n",
    "    // Set up the molecules to draw\n",
    "    var molecules = [];\n",
    "    for (i=0; i<8; i++) {\n",
    "        var col = (i < 4) ? \"#fe3\" : \"#b00\";\n",
    "        var y0 = (i < 4) ? 10 : canvas.height - 10;\n",
    "        molecules.push(molecule({x:(i+1)*10, y:y0, color:col}));\n",
    "    }\n",
    "    \n",
    "    // Set up the animation function\n",
    "    var counter = 0;\n",
    "    function draw() {\n",
    "        var i, j;\n",
    "        for (i=0; i < molecules.length; i++) {\n",
    "            if (counter < 10 | (counter > 1800 & counter < 1810)) {\n",
    "                for (j=0; j < molecules.length; j++) {\n",
    "                    molecules[j].x = molecules[j].x0;\n",
    "                    molecules[j].y = molecules[j].y0;\n",
    "                }\n",
    "            } else {\n",
    "                counter = (counter < 1810) ? counter : 0;\n",
    "                molecules[i].step();            \n",
    "            }\n",
    "        }\n",
    "        for (i=0; i < molecules.length; i++) {\n",
    "            for (j=i+1; j < molecules.length; j++) {\n",
    "                molecules[i].handleCollision(molecules[j]);\n",
    "            }\n",
    "            molecules[i].handleCollision(); // walls\n",
    "        }\n",
    "        ctx.beginPath();\n",
    "        ctx.fillStyle = 'white';\n",
    "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
    "        ctx.strokeRect(0, 0, canvas.width, canvas.height)\n",
    "        ctx.closePath();\n",
    "        ctx.lineWidth = 1;\n",
    "        for (i=0; i < molecules.length; i++) {\n",
    "            molecules[i].draw();\n",
    "        }\n",
    "        counter++;\n",
    "    }\n",
    "\n",
    "  // Animate\n",
    "  window.setInterval(draw, 20);\n",
    "})(); </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ising model\n",
    "\n",
    "The Ising model is named for Ernst Ising, a student of Wilhelm Lenz. He solved the 1-D problem for his doctoral thesis in 1920. In the model, a material is represented by a regular lattice of atoms that can have positive or negative dipole moment (magnetic spin $s$ is up or down: $s = \\pm 1$).\n",
    "\n",
    "For two or more dimensions, there is a Curie temperature $T_c$ at which a phase transition occurs. Below $T_c$, the substance behaves as if it were magnetic. Above it, thermal energy disrupts the tendency toward alignment, and the substance acts as a paramagnetic one.\n",
    "\n",
    "Here is an example drawing of a typical state below the critical temperature (left—all one spin—100% magnetization) and above the critical temperature (right—random spins—nearly 0% magnetization):\n",
    "\n",
    "<canvas id=\"mcmc-ising-two-lattices\" width=\"600px\" height=\"250px\" style=\"display:block;margin:1em auto 0;\">\n",
    "    Two 2-D lattices. One is all white, showing spins aligned. The other is randomly half blue and half white, showing spins not aligned.\n",
    "</canvas>\n",
    "\n",
    "#### Boltzmann distribution: where the MCMC comes in\n",
    "\n",
    "In the Ising model, the distribution of spins in the lattice depends on temperature, and follows the Boltzmann distribution:\n",
    "\n",
    "$$\n",
    "    \\text{frequency distribution} \\propto e^{-E / k T}\n",
    "$$\n",
    "\n",
    "In which $k$ is Boltzmann's constant, $T$ is the lattice temperature, and $E$ is the total energy from magnetization. When there is no external magnetic field, and given a coupling parameter $J > 0$ (a property of the particular material), the energy from magnetization is:\n",
    "\n",
    "$$\n",
    "   E = - J \\sum_{\\text{(atoms)}}\\sum_{\\text{(neighbor atoms)}}\n",
    "       \\left( s_{\\text{atom}} \\cdot s_{\\text{neighbor}}\\right)\n",
    "$$\n",
    "\n",
    "There's a shorthand notation:\n",
    "\n",
    "$$\n",
    "   E =  - J \\sum_{<i, j>} s_i s_j\n",
    "$$\n",
    "\n",
    "The subscript $<i, j>$ denotes a sum over each atom's $j$ nearest neighbors.\n",
    "\n",
    "The goal of the model is to identify  $T_c$. We do that by running the MCMC analysis a bunch of times at different temperatures to determine the average magnetization as a function of temperature. Then it's pretty clear where the discontinuity occurs.\n",
    "\n",
    "The solution in this post is from a [lecture by Richard Fitzpatrick](http://farside.ph.utexas.edu/teaching/329/lectures/node110.html) at the University of Texas. His lecture is understandable and thorough."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "// %%javascript\n",
    "(function(){\n",
    "    var canvas = document.getElementById(\"mcmc-ising-two-lattices\");\n",
    "    var ctx = canvas.getContext('2d');\n",
    "    var i, j, r=10, g=25, dx=canvas.width-g*10;\n",
    "    var twoPI = Math.PI * 2;\n",
    "    canvas.setAttribute(\"role\", \"image\");\n",
    "    canvas.setAttribute(\"aria-label\", canvas.innerHTML);\n",
    "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "    ctx.strokeStyle = \"black\";\n",
    "    ctx.fillStyle = \"#fff\";\n",
    "    ctx.font = \"12px sans-serif\";\n",
    "    ctx.textAlign = \"center\";\n",
    "    // grid\n",
    "    for (i=0; i <= 10; i++) {\n",
    "        ctx.beginPath();\n",
    "        ctx.moveTo(i*g,0); ctx.lineTo(i*g, g*10);\n",
    "        ctx.moveTo(0, i*g); ctx.lineTo(canvas.height, i*g);\n",
    "        ctx.moveTo(dx+i*g,0); ctx.lineTo(dx+i*g, canvas.height);\n",
    "        ctx.moveTo(dx, i*g); ctx.lineTo(canvas.width, i*g);\n",
    "        ctx.closePath();\n",
    "        ctx.stroke();\n",
    "    }\n",
    "    // molecules\n",
    "    for (i=0; i <= 10; i++) {\n",
    "        for (j=0; j<= 10; j++) {\n",
    "            // magnetized\n",
    "            ctx.beginPath();\n",
    "            ctx.arc(i*g, j*g, r, 0, twoPI);\n",
    "            ctx.stroke();\n",
    "            ctx.fill();\n",
    "            ctx.fillStyle = \"#000\";\n",
    "            ctx.fillText(\"↑\", i*g, j*g + r/2);\n",
    "            ctx.fillStyle = \"#fff\";\n",
    "            // not magnetized\n",
    "            ctx.beginPath();\n",
    "            ctx.arc(dx + i*g, j*g, r, 0, twoPI);\n",
    "            ctx.stroke();\n",
    "            if (Math.random() < .5) {\n",
    "                ctx.fillStyle = \"#33d\";\n",
    "                ctx.fill(); \n",
    "                ctx.fillStyle = \"#ccc\";\n",
    "                ctx.fillText(\"↓\", dx + i*g, j*g + r/2);\n",
    "                ctx.fillStyle = \"#fff\";\n",
    "            } else {\n",
    "                ctx.fill();\n",
    "                ctx.fillStyle = \"#000\";\n",
    "                ctx.fillText(\"↑\", dx + i*g, j*g + r/2);\n",
    "                ctx.fillStyle = \"#fff\";\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})();</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "Here are three approaches to calculating the equilibrium magnetization:\n",
    "\n",
    "### Sum over all states\n",
    "\n",
    "We can brute-force calculate the equilibrium magnetization of the\n",
    "system by iterating through every single possible state, calculating\n",
    "its energy given the magnetization, and then performing a weighted average based on its frequency, $\\exp(-E / k T)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{average magnetization} &= \\sum_{\\text{all configurations}}\n",
    "          \\text{magnetization} \\cdot \\text{frequency of configuration} \\\\\n",
    "          &= \\sum_{\\text{all configurations}}\n",
    "          \\text{magnetization} \\cdot e^{-E / k T}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But that's a lot of states. Even if we have a quite small $10 \\times 10$ grid, it makes 100 positions and thus—with spin up and spin down—$2^{100}$ total different configurations. At modern processor speeds, this is still a time prohibitive approach.\n",
    "\n",
    "### Weighted sum over randomly-chosen states\n",
    "\n",
    "So, rather than stepping through all of the possible configurations, what if we randomly choose a bunch of configurations and take the weighted average of their frequency? \n",
    "\n",
    "The problem is it's inefficient. The Boltzmann distribution is not uniform: it is weighted toward lower energies. Ideally, we'd find some way to pick spin configurations that correspond more often to the more frequently occurring states.\n",
    "\n",
    "<canvas id=\"mcmc-ising-Boltzmann-distribution\" width=\"350px\" height=\"250px\" style=\"display:block;margin:1em auto 0;\">\n",
    "    Plot of a section of the Boltzmann distribution near zero: the point is to show that, since it's a decreasing exponential function, the bulk of the distribution is concentrated near zero.\n",
    "</canvas>\n",
    "\n",
    "### Metropolis-Hastings method\n",
    "\n",
    "And that's precisely the contribution from nuclear physicist Nicholas Metropolis and his team of mathematicians [(Metropolis paper, paywalled)](https://aip.scitation.org/doi/10.1063/1.1699114). The method was originally published in 1953. They pioneered both the use of random-number generation for simulation (Monte Carlo methods) and the chaining of random steps (Markov Chain Monte Carlo) in their work for the Manhattan project.\n",
    "\n",
    "They use their newly created technique to take a random walk through the configuration space, visiting the more frequently-occurring spin configurations more often, and drastically speeding up the calculation. In their words:\n",
    "\n",
    "<blockquote>\n",
    " Instead of choosing configurations randomly,\n",
    " then weighting them with $\\exp(-E/kT)$, we choose\n",
    " configurations with a probability $\\exp(-E/kT)$ and\n",
    " weight them evenly.\n",
    "</blockquote>\n",
    "\n",
    "They never really explained the statistics behind why it worked, though. Just the physics of what was happening. That's why it's named Metropolis-Hastings: the '-Hastings' acknowledges the statistician W. K. Hastings's contribution in 1970, when the theory behind the technique was finally explained and generalized [(Hastings paper)](https://www.jstor.org/stable/2334940).\n",
    "\n",
    "\n",
    "I'm not a physicist or a statistician, so it's a testament to the writing talent of both Hastings and the Metropolis team that their papers are actually understandable to people with an average engineering background and a bit of stats. Hastings writes:\n",
    "\n",
    "<blockquote>\n",
    "The main features of these methods for sampling from a\n",
    "distribution with density $p(x)$ are:\n",
    "    <ol>\n",
    " <li type=\"a\">\n",
    "     The computations depend on $p(x)$ only through ratios\n",
    "     of the form $p(x')/p(x)$, where $x'$ and $x$ are sample\n",
    "     points. Thus, the normalizing constant need not be known,\n",
    "     no factorization of $p(x)$ is necessary, and the methods\n",
    "     are very easily implemented on a computer. [...]\n",
    "  <li type=\"a\">\n",
    "      A sequence of samples is obtained by simulating a\n",
    "      Markov chain. The resulting samples are therefore\n",
    "      correlated and estimation of the standard deviation\n",
    "      of an estimate and assessment of the error of an\n",
    "      estimate may require more care than with\n",
    "      independent samples. \n",
    "    </ol>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "// %%javascript\n",
    "(function(){\n",
    "    var canvas = document.getElementById(\"mcmc-ising-Boltzmann-distribution\");\n",
    "    var ctx = canvas.getContext('2d');\n",
    "    canvas.setAttribute(\"role\", \"image\");\n",
    "    canvas.setAttribute(\"aria-label\", canvas.innerHTML);\n",
    "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "\n",
    "    function makeMapping(domain, range) {\n",
    "        function mapping(input) {\n",
    "            return range[0] + (range[1] - range[0]) * input / (domain[1] - domain[0]);\n",
    "        }\n",
    "        return mapping;\n",
    "    }\n",
    "    var x = makeMapping([0, 5], [15, canvas.width-10]);\n",
    "    var y = makeMapping([0, 1], [canvas.height-15, 10]);\n",
    "    \n",
    "    // Axes\n",
    "    ctx.strokeStyle = \"black\";\n",
    "    ctx.lineWidth = 1;\n",
    "    ctx.beginPath();\n",
    "    ctx.moveTo(x(0), y(1));\n",
    "    ctx.lineTo(x(0), y(0));\n",
    "    ctx.lineTo(x(5), y(0));\n",
    "    ctx.stroke();\n",
    "    // Curve\n",
    "    ctx.strokeStyle = \"#58f\";\n",
    "    ctx.lineWidth = 3;\n",
    "    ctx.beginPath();\n",
    "    var prev = [x(.01), y(Math.exp(-0.01))];\n",
    "    ctx.moveTo(prev[0], prev[1]);\n",
    "    for (var i=2; i<100; i++) {\n",
    "        var e_kT = i/20;\n",
    "        var fDist = Math.exp(-i/20);\n",
    "        var next = [x(e_kT), y(fDist)];\n",
    "        var mid = [(prev[0]+next[0])/2, (prev[1]+next[1])/2];\n",
    "        ctx.bezierCurveTo(mid[0], mid[1], mid[0], mid[1], next[0], next[1]);\n",
    "        prev = next;\n",
    "    }\n",
    "    ctx.stroke()\n",
    "    // Annotation\n",
    "    ctx.font = \"14px sans-serif\";\n",
    "    ctx.fillStyle = \"#000\";\n",
    "    ctx.fillText(\"lower energy:\", x(.18), y(.93));\n",
    "    ctx.fillText(\"more often\", x(.21), y(.86));\n",
    "    ctx.fillText(\"higher energy:\", x(3.69), y(.13));\n",
    "    ctx.fillText(\"less often\", x(3.72), y(.06));\n",
    "    ctx.stroke();\n",
    "})();</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Hastings was saying\n",
    "\n",
    "It's easier to understand ideas with a concrete example like the Ising model. The important physics, described earlier, is that the spin configurations follow the Boltzmann distribution:\n",
    "\n",
    "$$\n",
    " \\text{frequency distribution of configurations} \\propto \\exp(-E / k T)\n",
    "$$\n",
    "\n",
    "This notation is too verbose, so let:\n",
    "- $x$ = the configuration of the lattice\n",
    "- $p$ = the frequency distribution of the configurations (the probability mass function)\n",
    "- $\\tilde{T}$ = dimensionless temperature $\\tilde{T} = k T / J$\n",
    "- $H(x)$ = the sum over neighboring spins $\\sum_{<ij>} s_i s_j$ for configuration $x$\n",
    "\n",
    "With the new symbols:\n",
    "\n",
    "$$\n",
    " p(x) \\propto \\exp(-H(x)/\\tilde{T})\n",
    "      = \\exp\\biggl(-\\frac{1}{\\tilde{T}}  \\sum_{<ij>} s_i s_j \\biggr)\n",
    "$$\n",
    "\n",
    "The reason we just say it's proportional to is because in order to know\n",
    "the probability that a specific spin configuration occurs, you need to divide it by all of the possible configurations so that the probabilities add up to one:\n",
    "\n",
    "$$\n",
    " p(x) =\n",
    "  \\frac{ \\exp(- H(x) / \\tilde{T}) }\n",
    "  { \\sum_{\\text{all }x} \\exp(- H(x) / \\tilde{T}) }\n",
    "$$\n",
    "\n",
    "But we don't know the denominator. Which was the original hangup for Bayesian statisticians. Fortunately, it's not a problem when you use MCMC. Like Hastings said,\n",
    "\n",
    "<blockquote>\n",
    "The computations depend on $p(x)$ only through ratios\n",
    "     of the form $p(x')/p(x)$,<wbr> where $x'$ and $x$ are sample\n",
    "     points. Thus, the normalizing constant need not be known,\n",
    "     no factorization of $p(x)$ is necessary, and the methods\n",
    "     are very easily implemented on a computer. \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "For the Ising model, the Metropolis-Hasting algorithm follows these steps:\n",
    "\n",
    "1. Pick a starting spin configuration $x$.\n",
    "2. Pick a new spin configuration $x'$.\n",
    "3. Make a random choice to take the new configuration or keep\n",
    "   the old one, weighted by the relative likelihoods of the\n",
    "   new configuration vs. the old one  $p(x')/p(x)$.\n",
    "4. Repeat #2 to #3 until the system converges.\n",
    "5. Except actually this one just goes $N$ steps instead of checking for convergence because all I want to do is make an animation of the transient part.\n",
    "\n",
    "\n",
    "The probability mass function for $x$ is\n",
    "\n",
    "$$\n",
    " p(x) =\n",
    "  \\frac{ \\exp(- H(x) / \\tilde{T}) }\n",
    "  { \\sum_{\\text{all }x} \\exp(- H(x) / \\tilde{T}) }\n",
    "$$\n",
    "\n",
    "which means that for step #3, the ratio of likelihoods is\n",
    "\n",
    "$$\n",
    " \\frac{p(x')}{p(x)} =\n",
    "  \\frac{ \\exp(- H(x') / \\tilde{T}) }\n",
    "       { \\sum_{\\text{all }x} \\exp(- H(x) / \\tilde{T}) } \\bigg/\n",
    "  \\frac{ \\exp(- H(x) / \\tilde{T}) }\n",
    "       { \\sum_{\\text{all }x} \\exp(- H(x) / \\tilde{T}) }\n",
    "$$\n",
    "\n",
    "It's a ratio, so we can cancel out the sum over all configurations\n",
    "\n",
    "$$\n",
    " \\frac{p(x')}{p(x)} =\n",
    " \\frac{ \\exp(- H(x') / \\tilde{T}) }\n",
    "      { \\exp(- H(x) / \\tilde{T})  }\n",
    "$$\n",
    "\n",
    "Rearranging, the ratio of likelihoods is:\n",
    "\n",
    "$$\n",
    " \\frac{p(x')}{p(x)} =\n",
    " \\exp\\left(- \\frac{H(x') - H(x)}{\\tilde{T}}\\right)\n",
    "$$\n",
    "\n",
    "And now so long as the algorithm selects the\n",
    "new configuration $x'$ relative to the old configuration $x$\n",
    "with the above proportion, we will walk the Markov Chain with\n",
    "appropriate weighted probabilities.\n",
    "Hastings describes a couple of options in the literature,\n",
    "but says the choice by Metropolis and his team was the most efficient.\n",
    "So, the algorithm is to switch to $x'$ from $x$ with the probability:\n",
    "\n",
    "$$\n",
    "p(\\text{switch to }x') = \\left\\lbrace\n",
    "\\begin{matrix}\n",
    "1 & \\text{ if }  H(x') - H(x) < 0\n",
    "  \\kern1em\\text{(lower energy)}\\\\\n",
    "\\exp\\left(- \\frac{H(x') - H(x)}{\\tilde{T}}\\right) & \\text{ if }  H(x') - H(x) \\ge 0\n",
    "  \\kern1em\\text{(higher energy)}  \n",
    "\\end{matrix}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "The code uses these Python libraries:\n",
    "\n",
    "```bash\n",
    "numpy mkl-service theano pymc3 array2gif\n",
    "```\n",
    "\n",
    "This first chunk of code has nothing to do with MCMC. It's for visualization: it converts an array of spin lattices (spin values $\\pm 1$) to blue and white and writes it to an animated gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from array2gif import write_gif\n",
    "\n",
    "\n",
    "def to_two_color(lattice):\n",
    "    blue = np.ones(lattice.shape, dtype=np.int) * 255 \n",
    "    red = np.zeros(lattice.shape, dtype=np.int)\n",
    "    red[lattice < 0] = 255 \n",
    "    green = red \n",
    "    return np.array([red, green, blue])\n",
    "\n",
    "\n",
    "def output_to_gif(dataset, filename, fps=8):\n",
    "    print(\"Frames: {}\".format(len(dataset)))\n",
    "    colors = []\n",
    "    write_gif(\n",
    "        [to_two_color(lattice) for lattice in dataset],\n",
    "        filename,\n",
    "        fps=fps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard approach\n",
    "\n",
    "The code in this section implements the Metropolis-Hastings method for the Ising model as\n",
    "[Richard Fitzpatrick describes it](http://farside.ph.utexas.edu/teaching/329/lectures/node110.html).\n",
    "The code about to calculate the energy (`get_dH`) appears first. After that,\n",
    "in `standard_approach`, comes the MCMC implementation.\n",
    "\n",
    "The function `get_dH` calculates $H(x') - H(x)$.\n",
    "Because none of the other positions in the lattice change,\n",
    "it is not necessary to calculate the energy except at the\n",
    "location $i,j$ of the flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dH(lattice, trial_location):\n",
    "    \"\"\" H = - Sum_<ij>(s_i s_j) \"\"\"\n",
    "    i, j = trial_location\n",
    "    height, width = lattice.shape\n",
    "    H, Hflip = 0, 0\n",
    "    for di, dj in ((-1, 0), (1, 0), (0, -1), (0, 1)):\n",
    "        ii = (i + di) % height\n",
    "        jj = (j + dj) % width\n",
    "        H -= lattice[ii, jj] * lattice[i, j]\n",
    "        Hflip += lattice[ii, jj] * lattice[i, j]\n",
    "    return Hflip - H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `standard_approach` creates a lattice with values\n",
    "$\\pm 1$, and loops through all positions $N * 5$ times, taking\n",
    "a snapshot every 5<sup>th</sup> step.\n",
    "\n",
    "It's not very random to step through every position in the\n",
    "lattice sequentially...but it's faster than randomly\n",
    "selecting positions because you are certain to visit every position in the fewest possible steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_approach(T, width, height, N=60):\n",
    "    # Randomly initialize the spins to either +1 or -1\n",
    "    lattice = 2 * np.random.randint(2, size=(height, width)) - 1\n",
    "    snapshots = []\n",
    "    for snapshot in range(N):\n",
    "        snapshots.append(to_two_color(lattice))\n",
    "        print('{:2.0%} complete. Net magnetization: {:3.0%}'\n",
    "              .format(snapshot / N,\n",
    "                      abs(lattice.sum()) / lattice.size),\n",
    "              end='\\r')\n",
    "        for step in range(5):\n",
    "            # Walk through the array flipping atoms.\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    dH = get_dH(lattice, (i, j))\n",
    "                    if dH < 0:  # lower energy: flip for sure\n",
    "                        lattice[i, j] = -lattice[i, j]\n",
    "                    else:  # Higher energy: flip sometimes\n",
    "                        probability = np.exp(-dH / T)\n",
    "                        if np.random.rand() < probability:\n",
    "                            lattice[i, j] = -lattice[i, j]\n",
    "    return snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyMC3\n",
    "\n",
    "The code in this section implements the Metropolis-Hastings method for the Ising model using PyMC3.\n",
    "\n",
    "The function `get_H` calculates the full $H(x)$ for the lattice, because in the PyMC3\n",
    "implementation, each step is actually completely independent of the next (it's supposed to be),\n",
    "meaning I can't access the prior state's lattice configuration to calculate the difference\n",
    "`get_dH`, $H(x') - H(x)$, like in the custom code.\n",
    "\n",
    "The lattice in this case is a Bernoulli random variable, so its\n",
    "values are $\\lbrace 0, 1\\rbrace$ not $\\lbrace -1, +1\\rbrace$.\n",
    "That's the reason for the `to_spins` function, which converts\n",
    "to $\\lbrace -1, +1\\rbrace$. Then `get_H` uses Theano for GPU\n",
    "matrix math (when available); in this case to shift the matrix indices\n",
    "by one to get the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "from theano.compile.ops import as_op\n",
    "\n",
    "# Fix compile failure on OSX\n",
    "# https://stackoverflow.com/a/51312739\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "\n",
    "def get_H(spins):\n",
    "    H = - (\n",
    "        tt.roll(spins, 1, axis=1) * spins +\n",
    "        tt.roll(spins, 1, axis=0) * spins +\n",
    "        tt.roll(spins, -1, axis=1) * spins +\n",
    "        tt.roll(spins, -1, axis=0) * spins\n",
    "    )\n",
    "    return H\n",
    "\n",
    "\n",
    "def to_spins(lattice):\n",
    "    return 2 * lattice - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way PyMC3 is used here is nonstandard: typically you'd use observed values to update the <span class=\"vocabulary\" title=\"The prior is a probability distribution that reflects all of the previous knowledge of the system to this point. Often the uniform (flat) distribution, which implies no previous knowledge.\">prior</span> estimate of the variables you're looking for, but this example has no observed values. Instead, it adds the `pm.Potential`, which will change the <span class=\"vocabulary\" title=\"Likelihood is what statisticians rename the probability mass function (or density function) when you fix the the observed value (usually the `x`) and instead vary distribution parameters (such as `theta`). In equation form, instead of p(x|theta) they write it L(theta) = p(theta|x).\">likelihood</span> directly to guide the MCMC random walk toward the lower-energy configurations.\n",
    "\n",
    "The function `mc3_approach` runs the MCMC simulation using PyMC3. It performs th same function as `standard_approach`, for the most part. The main difference is it uses a full-blown MCMC framework, so we can't cheat and step through the positions in sequence like we did with the custom code.\n",
    "\n",
    "Instead, at each step it will draw an entirely new lattice to try. The problem with this is multiple spins may cancel each other out, meaning the MCMC algorithm will take longer to converge. To fix this, we set `scaling = .0006` so that on average only one position in the lattice will change at each step. (From the source code, a random uniform value must be below   `1 - .5**scaling` for the algorithm to attempt a flip.)\n",
    "\n",
    "Here's the function. Each line will be broken down in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc3_approach(T, width, height, N=100):\n",
    "    shape = (height, width)\n",
    "    x0 = np.random.randint(2, size=shape)\n",
    "    with pm.Model() as model:\n",
    "        x = pm.Bernoulli('x', 0.5, shape=shape, testval=x0)\n",
    "        magnetization = pm.Potential(\n",
    "            'm',\n",
    "            -get_H(to_spins(x)) / T\n",
    "        )\n",
    "        scaling = .0006\n",
    "        mul = int(height * width * 1.75)\n",
    "        step = pm.BinaryMetropolis([x], scaling=scaling)\n",
    "        trace = pm.sample(N * mul * 5, step=step, chains=1, tune=False)\n",
    "    dataset = [to_two_color(2 * t['x'] - 1) for t in trace[::mul * 5]]\n",
    "    # Print out the final percent magnetization\n",
    "    lattice = 2 * trace[-1]['x'] - 1\n",
    "    print('Finished. Net magnetization: {:3.0%}'\n",
    "              .format(abs(lattice.sum()) / lattice.size))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The middle part is where the PyMC3 library is used. The code below ",
    "is the same as above starting from the PyMC3 model context (`with pm.Model() as model`), with added comments.\n",
    "\n",
    "```python\n",
    "# PyMC3 uses a model context to collect all of the\n",
    "# random variables together. Every random variable that\n",
    "# is declared inside this context will be attached to\n",
    "# the model. The variables can depend on each other,\n",
    "# and will advance through the Markov Chain together.\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # PyMC3 has pre-defined common discrete and continuous\n",
    "    # distributions. The Bernoulli distribution has values\n",
    "    # that are in {0, 1}.\n",
    "    x = pm.Bernoulli('x', 0.5, shape=shape, testval=x0)\n",
    "    \n",
    "    # The Potential function is summed and added to the\n",
    "    # overall log-likelihood. PyMC3 uses log-likelihood\n",
    "    # instead of likelihood so when they take p(x')/p(x)\n",
    "    # they can do it with subtraction:\n",
    "    #   log( p(x') / p(x)) = log(p(x')) - log(p(x))\n",
    "    #\n",
    "    # So, long story--this line plays the same role as\n",
    "    # the `get_dH` function earlier.\n",
    "    magnetization = pm.Potential('m', -get_H(to_spins(x))/ T)\n",
    "    \n",
    "    # `scaling` limits the fraction of positions in `x`\n",
    "    # that flip at every step in the Markov Chain.\n",
    "    # From the PyMC3 source:\n",
    "    #     p_jump = 1. - .5 ** self.scaling\n",
    "    #     ...\n",
    "    #     switch_locs = (rand_array < p_jump)\n",
    "    scaling = .0006\n",
    "    \n",
    "    # `mul` was originally height * width to make the same\n",
    "    # number of iterations as in the standard approach.\n",
    "    # I made it bigger because the trace didn't converge.\n",
    "    mul = int(height * width * 1.75)\n",
    "    \n",
    "    # PyMC3 has a lot of different step options, including\n",
    "    # No U-Turn sampling (NUTS), slice, and Hamiltonian\n",
    "    # Monte-Carlo. The Metropolis method is the oldest;\n",
    "    # the others may converge faster depending on the\n",
    "    # application.\n",
    "    step = pm.BinaryMetropolis([x], scaling=scaling)\n",
    "    \n",
    "    # `trace` is the object that contains all of the steps.\n",
    "    # When there are multiple random variables in the model,\n",
    "    # it will contain all of those variables.\n",
    "    trace = pm.sample(N * mul * 5,\n",
    "                      step=step,\n",
    "                      chains=1,  # default is 2\n",
    "                      tune=False)\n",
    "```\n",
    "\n",
    "\n",
    "### Now run both approaches\n",
    "Finally, this is the part where the two different methods are called\n",
    "and compared. I had to increase the number of samples in the MC3 approach\n",
    "because the random choices weren't as efficient as stepping through the\n",
    "lattice. Also, I chose numbers far above and below the critical ratio\n",
    "so that the test runs would converge faster with this larger matrix size.\n",
    "\n",
    "You would normally run this on a 10x10 or maybe 20x20 lattice, and run for many more steps to get better convergence, but blog posts need visuals, and I wanted an animation of the transient state. I couldn't make the lattice bigger because I was running into memory issues for the PyMC3 method (which stores every single value in the `trace`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98% complete. Net magnetization:  97%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "BinaryMetropolis: [x]\n",
      "100%|██████████| 1750000/1750000 [24:57<00:00, 1168.24it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Net magnetization: 100%\n",
      "98% complete. Net magnetization:   2%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "BinaryMetropolis: [x]\n",
      "100%|██████████| 1750000/1750000 [21:42<00:00, 1343.42it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Net magnetization:   0%\n"
     ]
    }
   ],
   "source": [
    "def run(T_over_Tc=.9, width=50, height=50, mc3=False):\n",
    "    Tc = 2.269  # Normalized T := kT/J\n",
    "    T = T_over_Tc * Tc\n",
    "    dataset = None\n",
    "    if mc3:\n",
    "       dataset = mc3_approach(T, width, height, N=80)\n",
    "       filename = ('mc3_ising_{}_{}x{}.gif'\n",
    "                   .format(T_over_Tc, width, height))\n",
    "    else:\n",
    "       dataset = standard_approach(T, width, height, N=60)\n",
    "       filename = ('ising_{}_{}x{}.gif'\n",
    "                   .format(T_over_Tc, width, height))\n",
    "    write_gif(dataset, filename, fps=8)\n",
    "\n",
    "run(T_over_Tc=.75)\n",
    "run(T_over_Tc=.75, mc3=True)\n",
    "run(T_over_Tc=1.25)\n",
    "run(T_over_Tc=1.25, mc3=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "Convergence is faster with the custom code because the 'random walk' was actually stepping through each lattice position in sequence. Still, it's possible to see the system settling in to magnetization for $T = 0.75 T_c$ in both implementations. And even though in the PyMC3 implementation, the higher temperature system looks a lot less active than in the custom implementation, the net magnetization in both is near zero, with equal amounts of spin up and down.\n",
    "\n",
    "<div style=\"display:flex;justify-content:space-around;\">\n",
    "    <figure style=\"margin:1em\">\n",
    "        <figcaption>Standard method<br/>$T = 0.75 T_c$</figcaption>\n",
    "        <img src=\"ising_0.75_50x50.gif\" style=\"width:100px;height:100px\"/>\n",
    "    </figure><figure style=\"margin:1em\">\n",
    "        <figcaption>Standard method<br/>$T = 1.25 T_c$</figcaption>\n",
    "        <img src=\"ising_1.25_50x50.gif\" style=\"width:100px;height:100px\"/>\n",
    "    </figure><figure style=\"margin:1em\">\n",
    "        <figcaption>pymc3 method:<br/>$T = 0.75 T_c$</figcaption>\n",
    "        <img src=\"mc3_ising_0.75_50x50.gif\" style=\"width:100px;height:100px\"/>\n",
    "    </figure><figure style=\"margin:1em\">\n",
    "        <figcaption>pymc3 method:<br/>$T = 1.25 T_c$</figcaption>\n",
    "      <img src=\"mc3_ising_1.25_50x50.gif\" style=\"width:100px;height:100px\"/>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The PyMC3 library provides a ready-made framework for applying Markov Chain Monte Carlo methods in Bayesian inference.\n",
    "\n",
    "- If you're an engineer, it's not bad to think of MCMC as a generalized\n",
    "  method for creating \"thermodynamic ensembles\" of any real-world variable.\n",
    "- The system steps through probability space, with a weighted random\n",
    "  walk that's biased toward the more likely states.\n",
    "- At the end, you will have a sample trace that contains the\n",
    "  samples from the random walk. The weighting of the walk is such that\n",
    "  if you randomly draw new predictions from the tail end of the trace\n",
    "  (after it has converged), your draws will follow the posterior\n",
    "  distribution of your target variable.\n",
    "- It uses Theano under the hood. Theano is a matrix math library that can leverage the NVIDIA\n",
    "  GPU. The caveat is that custom code requires using Theano's functions for\n",
    "  [tensor math](http://deeplearning.net/software/theano/library/tensor/basic.html).\n",
    "  Note that [PyMC4](https://github.com/pymc-devs/pymc4)\n",
    "  is about to come out and it depends on TensorFlow if you prefer that\n",
    "  to Theano.\n",
    "  \n",
    "PyMC3 also implements No U-Turn Sampling (NUTS) and Hamiltonian Monte Carlo methods. They are modern MCMC techniques that speed up convergence in some cases by using different weights on the random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was created on an x86_64 computer running OSX 10.11.6 and using:\n",
      "Python 3.6.5\n",
      "IPython 6.4.0\n",
      "mkl 1.1.2\n",
      "NumPy 1.14.5\n",
      "PyMC3 3.5.rc1\n",
      "Theano 1.0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, IPython, numpy as np, mkl, theano, pymc3 as pm\n",
    "print(\"This notebook was created on an %s computer running OSX %s and using:\\nPython %s\\nIPython %s\\nmkl %s\\nNumPy %s\\nPyMC3 %s\\nTheano %s\\n\" % (platform.machine(), platform.mac_ver()[0], sys.version[:5], IPython.__version__, mkl.__version__, np.__version__, pm.__version__, theano.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "\n",
    "- **Likelihood**<br/>\n",
    "  This is what statisticians call the probability mass (or density)\n",
    "  function $p(x|\\theta)$ when, instead of evaluating it\n",
    "  for fixed parameters $\\theta$ they evaluate it for\n",
    "  fixed observation $x$ and allow the parameters themselves\n",
    "  to vary across their entire domain. To make this more explicit\n",
    "  when talking about it, they give the function a different\n",
    "  name: the likelihood $L(\\theta|x) \\equiv p(\\theta|x)$.\n",
    "  The total area under the likelihood need not\n",
    "  equal one, so it must be normalized.\n",
    "\n",
    "- **Monte Carlo Method**<br/>\n",
    "  The practice of using pseudo-random numbers generated in a\n",
    "  computer to model random events. Named for the Monte Carlo\n",
    "  casino in Monaco, where Stanislaw Ulam's uncle would gamble.\n",
    "  (Ulam is a coauthor on the 1953 Metropolis paper.)\n",
    "\n",
    "- **Markov Chain**<br/>\n",
    "  A sequence of events that each satisfy the Markov property:\n",
    "  that whatever happens next only depends on the current state, and\n",
    "  is independent of prior events. A Brownian motion is one example.\n",
    "  It is named for the Russian mathematician Andrey Andreevich Markov.\n",
    "\n",
    "- **Markov Chain Monte Carlo (MCMC)**<br/>\n",
    "  A sequence of successive Monte Carlo draws in which the starting\n",
    "  point of the current draw is the outcome of the last draw. The chain\n",
    "  steps through points in probability space. The MCMC algorithms have\n",
    "  a weighted preference for more likely outcomes, so the chain will\n",
    "  spend more of its time in the more likely regions. This means\n",
    "  the tail of the resulting Markov Chain will approximate the\n",
    "  posterior distribution, and you can draw from it like you\n",
    "  would draw from any computer-generated random distribution.\n",
    "  \n",
    "- **Posterior**<br/>\n",
    "  In the context of Bayesian statistics, this is a new distribution\n",
    "  that combines what you knew a priori with the observed data.\n",
    "  When using MCMC, the posterior is approximated by the tail end\n",
    "  of the sample trace.\n",
    "  Mathematically, you obtain the posterior from Bayes' theorem, in which\n",
    "  $p(x)$ is a prior of your choice, often constant, $x_\\text{obs}$\n",
    "  is the observed data, and $\\mathscr{L}$ is a likelihood of your choice.\n",
    "  $$P(x|x_\\text{obs}) = \\frac{P(x_\\text{obs} | x)}{P(x_\\text{obs})} p(x) = \\frac{\\mathscr{L}(x_\\text{obs}| x)}{\\int_{x}\\mathscr{L}(x_\\text{obs}|x) p(x) dx} p(x)$$\n",
    "  \n",
    "- **Prior**<br/>\n",
    "  In the context of Bayesian statistics, a distribution that reflects\n",
    "  what you know about a random variable prior to incorporating your\n",
    "  observations. Often people will just say the prior is uniform,\n",
    "  $p(x) \\propto 1$, and normalize it out. Robert Cousins gives the example\n",
    "  in particle physics where the prior can be zero for negative\n",
    "  mass and uniform otherwise, or other choices that make sense for the\n",
    "  context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "\n",
    "The following resources (in chronological order) are for people interested in the history of MCMC methods:\n",
    "\n",
    "- [Equation of State Calculations by Fast Computing Machines (paywall)](https://aip.scitation.org/doi/10.1063/1.1699114)<br/>\n",
    "Nicholas Metropolis and collaborators' paper starting it all.\n",
    "<em>J. Chem. Phys.</em>, Vol. 21, No. 6 (June 1953) pp. 1087-1092.\n",
    "- [Monte Carlo Sampling Methods Using Markov Chains and Their Applications](https://www.jstor.org/stable/2334940)<br/>\n",
    "Wilhelm Hastings's paper explaining and generalizing Metropolis's algorithm.\n",
    "<em>Biometrika</em>, Vol. 57, No. 1 (April 1970), pp. 97-109.\n",
    "- [The Beginning of the Monte Carlo Method (pdf)](https://library.lanl.gov/cgi-bin/getfile?00326866.pdf)<br/>\n",
    "A delightful historical account, by Nicholas Metropolis, written as part of a special newsletter honoring the scientific contributions of Stan Ulam. <em>Los Alamos Science</em>, Special Issue, 1987, pp.125-130.\n",
    "- [From EM to Data Augmentation: The Emergence of MCMC Bayesian Computation in the 1980s (pdf)](https://arxiv.org/pdf/1104.2210.pdf)<br/>\n",
    "Martin Tanner and Wing Wong's fantastic survey of the history of Markov Chain Monte Carlo methods.\n",
    "<em>Stat. Sci.</em>, Vol. 25, No. 4 (2010) pp. 506–516\n",
    "\n",
    "\n",
    "I'm new to the Bayesian approach. The following resources (in descending order of usefulness) have helped me:\n",
    "\n",
    "- [\"Why isn't every physicist a Bayesian?\" (pdf)](https://www.astro.princeton.edu/~strauss/AST303/bayesian_paper.pdf)<br/>\n",
    "☆ Choose this if you only have time for one paper. ☆<br/>\n",
    "Robert D. Cousins's paper is technical but understandable. It mercifully avoids the religious fervor of other papers on the topic.\n",
    "<em>Am. J. Phys.</em>, Vol. 63, No. 5 (May 1995).\n",
    "- [Journal Article Reporting Standards for Quantitative Research in\n",
    "Psychology: The APA Publications and Communications Board Task\n",
    "Force Report](https://www.apa.org/pubs/journals/releases/amp-amp0000191.pdf)<br/>\n",
    "Table 8 is about reporting Bayesian results. It's longer and more detailed than the BaSiS group checklist listed next.\n",
    "- [Standards for Reporting of Bayesian Analyses in the Scientific Literature (no https)](http://lib.stat.cmu.edu/bayesworkshop/2001/BaSis.html)<br/>\n",
    "The Bayesian Standards in Science group (BaSiS) compiled a checklist in 2001 for what to include when reporting Bayesian results. Short and sweet. [The actual checklist (no https)](http://lib.stat.cmu.edu/bayesworkshop/2001/BaSisGuideline.htm).\n",
    "- [Frequentism and Bayesianism: A Python-driven Primer (pdf)](https://arxiv.org/abs/1411.5018)<br/>\n",
    "Jake VanderPlas's 2014 paper is a summary of both the Frequentist and Bayesian philosophies. It uses accessible language and examples, and demonstrates both techniques using Python but sort of feels like it's trolling Frequentists. The Cousins paper was easier for me to understand. <em>Proc. of the 13th Python in science conf.</em> (SCIPY 2014).\n",
    "- [Frequentism and Bayesianism blog post](https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)<br/>\n",
    "VanderPlas's 2014 blog post describes PyMC (the precursor to [PyMC3](https://docs.pymc.io/)) and two other Python tools, with a lot of the examples eventually going into the above paper; for people who prefer a blog format.\n",
    "- [MCMC sampling for dummies](https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)<br/>\n",
    "Thomas Wiecki's 2015 blog post that inspired this one. It's good for people who are comfortable with the statistics part of the Bayesian approach but not comfortable with the numerical methods part of MCMC. (That's actually the opposite of what I was.)\n",
    "\n",
    "\n",
    "_All links in this post were accessed on or before July 29, 2018._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nikola": {
   "category": "Methods",
   "date": "2018-07-29 00:00:42 UTC-05:00",
   "description": "The Metropolis-Hastings method using both PyMC3 and standard techniques, demonstrated via the Ising model.",
   "link": "",
   "previewimage": "spins.png",
   "slug": "mcmc-and-the-ising-model",
   "tags": "pymc3,mcmc,bayesian",
   "title": "MCMC and the Ising Model",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
